{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = r\"D:\\NLP_Project\\HuggingFace\"\n",
    "os.environ['HOMEDRIVE'] = \"D:\"\n",
    "\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import GenerationConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM,Idefics2ForConditionalGeneration, Idefics2Processor,BitsAndBytesConfig\n",
    "import peft\n",
    "from peft import PeftModel, PeftConfig\n",
    "from peft import LoftQConfig, LoraConfig, get_peft_model, AutoPeftModel\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_trans_sr_en = \"facebook/nllb-200-distilled-1.3B\"\n",
    "model_vqa_name = \"HuggingFaceM4/idefics2-8b\"\n",
    "cache_dir = r\"D:\\NLP_Project\\HuggingFace\"\n",
    "model_name = \"ZacJQ/idefics2-8b-docvqa-finetuned-museum-v2\"\n",
    "peft_model_id = \"ZacJQ/idefics2-8b-docvqa-finetuned-museum-v2\"\n",
    "model_trans_en_sr = \"\"\n",
    "image_split = False\n",
    "max_len_token_trans = 800\n",
    "context_window_turns = 5\n",
    "no_turns = 1\n",
    "global chat_history\n",
    "chat_history = []\n",
    "source_lang = \"mar_Deva\"\n",
    "language = ['Bhojpuri', 'Gujarati', 'Hindi', 'Marathi', 'Urdu', 'English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c287ab2a11433a8333e162c381d7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "d:\\NLP_Project\\NLP\\Lib\\site-packages\\transformers\\integrations\\peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initializes the models\n",
    "\"\"\"\n",
    "global device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "                                            load_in_4bit=True,\n",
    "                                            bnb_4bit_quant_type=\"nf4\",\n",
    "                                            bnb_4bit_use_double_quant=True,\n",
    "                                            bnb_4bit_compute_dtype=torch.float16\n",
    "                                            )\n",
    "else:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "                                            load_in_4bit=True,\n",
    "                                            bnb_4bit_quant_type=\"nf4\",\n",
    "                                            bnb_4bit_use_double_quant=True,\n",
    "                                            bnb_4bit_compute_dtype=torch.float32\n",
    "                                            )\n",
    "# model_base = AutoModelForVision2Seq.from_pretrained(model_vqa_name,\n",
    "#                                                         torch_dtype=torch.float16,\n",
    "#                                                         quantization_config=quantization_config,\n",
    "#                                                         cache_dir=cache_dir,\n",
    "#                                                         )\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model_base = Idefics2ForConditionalGeneration.from_pretrained(config.base_model_name_or_path, \n",
    "                                                    cache_dir=cache_dir,\n",
    "                                                    torch_dtype=torch.float16,\n",
    "                                                    quantization_config=quantization_config,\n",
    "                                                    )\n",
    "# model_vqa = AutoPeftModel.from_pretrained(model_base, \n",
    "#                                       peft_model_id)\n",
    "\n",
    "processor_vqa = Idefics2Processor.from_pretrained(config.base_model_name_or_path,\n",
    "                                                # model_name,\n",
    "                                                do_image_splitting=False,\n",
    "                                                cache_dir=cache_dir \n",
    "                                                )\n",
    "\n",
    "\n",
    "model_vqa = model_base\n",
    "model_vqa.add_adapter(adapter_config=config, adapter_name=\"Finetuned\")\n",
    "model_vqa.enable_adapters()\n",
    "print(model_vqa.active_adapter())\n",
    "tokenizer_sr_en = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", cache_dir=cache_dir)\n",
    "model_trans_sr_en = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", quantization_config=quantization_config ,cache_dir=cache_dir)\n",
    "model_language = \"eng_Latn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_n_conversation_turns(message: dict, no_turns: int):\n",
    "    \"\"\"\n",
    "    Retrieve the last n conversation turns from the given messages.\n",
    "\n",
    "    Args:\n",
    "    messages (list): List of conversation messages.\n",
    "    n (int): Number of conversation turns to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    list: List of the last n conversation turns.\n",
    "    \"\"\"\n",
    "    no_turns = min(no_turns, len(message))\n",
    "    conversation_turns = message[-no_turns:]\n",
    "    return conversation_turns\n",
    "\n",
    "def get_text(grad_message: dict)-> str:\n",
    "    \"\"\"\n",
    "    Returns the current message from the user User\n",
    "    \"\"\"\n",
    "    user_input = grad_message['text']\n",
    "    return user_input\n",
    "\n",
    "def get_image(grad_message: dict)-> tuple[list,int]:\n",
    "    \"\"\"\n",
    "    Returns the list of images and no. of images from the User\n",
    "    \"\"\"\n",
    "    user_image = grad_message['files']\n",
    "    print(user_image)   # added for debugging\n",
    "    no_image = len(user_image)\n",
    "    return (user_image, no_image)\n",
    "\n",
    "def get_translation(text: str, source_lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts input query into the target LLM language (English)\n",
    "    \"\"\"\n",
    "    source_lang = \"mar_Deva\"  # Hard coded for now\n",
    "    task = \"translation\"  # Hard coded for now\n",
    "    translator = pipeline(task,\n",
    "                          model= model_trans_sr_en,\n",
    "                          tokenizer= tokenizer_sr_en,\n",
    "                          src_lang=source_lang,\n",
    "                          tgt_lang= model_language,\n",
    "                          max_length = max_len_token_trans\n",
    "                          )\n",
    "    output = translator(text)\n",
    "    trans_text_sr_en = output[0][\"translation_text\"]\n",
    "    trans_text_sr_en = trans_text_sr_en\n",
    "    print(\"Marathi to english - \",trans_text_sr_en)\n",
    "    return trans_text_sr_en\n",
    "\n",
    "def return_translation(text:str ,source_lang: str)-> str:\n",
    "    \"\"\"\n",
    "    Converts LLM output (English) to the original language\n",
    "    \"\"\"\n",
    "    source_lang = \"mar_Deva\"  # Hard coded for now\n",
    "    task = \"translation\"  # Hard coded for now\n",
    "    translator = pipeline(task,\n",
    "                          model= model_trans_sr_en,\n",
    "                          tokenizer= tokenizer_sr_en,\n",
    "                          src_lang=model_language,\n",
    "                          tgt_lang=source_lang ,\n",
    "                          max_length = max_len_token_trans\n",
    "                          )\n",
    "    output = translator(text)\n",
    "    trans_text_en_sr = output[0][\"translation_text\"]\n",
    "    print(\"Englsih to Marathi-\",trans_text_en_sr)\n",
    "    return trans_text_en_sr\n",
    "\n",
    "def get_template_user(grad_message: dict , chat_history: list)-> list:\n",
    "    \"\"\"\n",
    "    Converts the input message from user into template and appends to chat history\n",
    "    \"\"\"\n",
    "    text_sr = get_text(grad_message)\n",
    "    text = get_translation(text_sr, \"mar_Deva\")\n",
    "\n",
    "    image_list, no_image = get_image(grad_message)\n",
    "    if no_image == 0:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text},]})\n",
    "    else:\n",
    "        chat_history.append({\"role\": \"user\",\"content\": [{\"type\": \"image\"},{\"type\": \"text\", \"text\": text},]})\n",
    "    return chat_history\n",
    "\n",
    "def get_template_assistant(output_llm: str, chat_history: list)-> list:\n",
    "    \"\"\"\n",
    "    Converts the LLM output into the the given template and appends to chat history\n",
    "    \"\"\"\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": output_llm},]})\n",
    "    return chat_history\n",
    "\n",
    "def give_output(output: list)-> str:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    temp = output[0]\n",
    "    extra = \"Assistant:\"\n",
    "    assistant_reply = temp.split(extra)\n",
    "    return assistant_reply[-1]\n",
    "\n",
    "def print_like_dislike(x: gr.LikeData):\n",
    "    print(x.index, x.value, x.liked)\n",
    "\n",
    "def get_chat_history(history: list)-> list:\n",
    "    \n",
    "    return chat_history\n",
    "\n",
    "def chat_engine(chat_history: list,image: Image, max_new_token: int) -> str:\n",
    "    \"\"\"\n",
    "    Generates assistant replies to given input\n",
    "    \"\"\"\n",
    "\n",
    "    gen_config = GenerationConfig(do_sample= True, temperature= 1.15, num_beams=3, repetition_penalty= 1.4, top_p=0.97)\n",
    "    prompt = processor_vqa.apply_chat_template(chat_history,\n",
    "                                                    add_generation_prompt=True)\n",
    "    if image!= None:\n",
    "        inputs = processor_vqa(text=prompt,\n",
    "                                    images=[image],\n",
    "                                    return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        start = time.time()\n",
    "        generated_ids = model_vqa.generate(**inputs, max_new_tokens=max_new_token, generation_config=gen_config)\n",
    "        print(\"time for generations:\", (time.time() - start))\n",
    "        print(\"max memory allocated:\", (torch.cuda.max_memory_allocated())/1024*1024)\n",
    "        print(\"number of tokens generated:\", len(generated_ids[:,\n",
    "                                                            inputs[\"input_ids\"].size(1):][0]\n",
    "                                                                ))\n",
    "        output = processor_vqa.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        # print(processor_vqa.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "        output_reply = give_output(output)\n",
    "        print(output_reply)\n",
    "        return output_reply\n",
    "    else:\n",
    "        inputs = processor_vqa(text=prompt,\n",
    "                                    return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        start = time.time()\n",
    "        generated_ids = model_vqa.generate(**inputs, max_new_tokens=max_new_token, generation_config=gen_config)\n",
    "        print(\"time for generations:\", (time.time() - start))\n",
    "        print(\"max memory allocated:\", (torch.cuda.max_memory_allocated())/1024*1024)\n",
    "        print(\"number of tokens generated:\", len(generated_ids[:,\n",
    "                                                            inputs[\"input_ids\"].size(1):][0]\n",
    "                                                                ))\n",
    "        output = processor_vqa.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        # print(processor_vqa.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "        output_reply = give_output(output)\n",
    "        print(output_reply)\n",
    "        return output_reply   # changed this for testing\n",
    "    \n",
    "def gradio_interface_llm(current_message_gradio: dict, history: any, language: str):\n",
    "    \"\"\"\n",
    "    Uses all functions defined above and \n",
    "    \"\"\"\n",
    "    global chat_history\n",
    "    try:\n",
    "        if len(chat_history) == 0:\n",
    "            input = get_template_user(current_message_gradio, chat_history)\n",
    "            input = get_last_n_conversation_turns(input,7)\n",
    "            image, no_image = get_image(current_message_gradio)\n",
    "            if no_image == 0:\n",
    "                output_en = chat_engine(input, None ,max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            else:\n",
    "                output_en = chat_engine(input, image, max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            history = get_template_assistant(output_en,chat_history)    # changed this for testing get_template_assistant(output,chat_history) \n",
    "            chat_history = history\n",
    "            print(\"1\")\n",
    "        else:\n",
    "            input = get_template_user(current_message_gradio, chat_history)\n",
    "            input = get_last_n_conversation_turns(input,7)\n",
    "            image, no_image = get_image(current_message_gradio)\n",
    "            if no_image == 0:\n",
    "                output_en = chat_engine(input, None ,max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            else:\n",
    "                output_en = chat_engine(input, image, max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            history = get_template_assistant(output_en,chat_history)   # changed this for testing get_template_assistant(output,chat_history) \n",
    "            chat_history = history\n",
    "            print(\"2\")\n",
    "        for i in range(len(output)):   # changed this for testing\n",
    "            time.sleep(0.05)\n",
    "            yield output[:i+1]\n",
    "    except Exception as e:\n",
    "        chat_history = []\n",
    "        if len(chat_history) == 0:\n",
    "            input = get_template_user(current_message_gradio, chat_history)\n",
    "            input = get_last_n_conversation_turns(input,7)\n",
    "            image, no_image = get_image(current_message_gradio)\n",
    "            if no_image == 0:\n",
    "                output_en = chat_engine(input, None ,max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            else:\n",
    "                output_en = chat_engine(input, image, max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            history = get_template_assistant(output_en,chat_history)    # changed this for testing get_template_assistant(output,chat_history) \n",
    "            chat_history = history\n",
    "            print(\"1\")\n",
    "        else:\n",
    "            input = get_template_user(current_message_gradio, chat_history)\n",
    "            input = get_last_n_conversation_turns(input,7)\n",
    "            image, no_image = get_image(current_message_gradio)\n",
    "            if no_image == 0:\n",
    "                output_en = chat_engine(input, None ,max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            else:\n",
    "                output_en = chat_engine(input, image, max_new_token=512)\n",
    "                output = return_translation(output_en, source_lang)\n",
    "            history = get_template_assistant(output_en,chat_history)   # changed this for testing get_template_assistant(output,chat_history) \n",
    "            chat_history = history\n",
    "            print(\"2\")\n",
    "        for i in range(len(output)):   # changed this for testing\n",
    "            time.sleep(0.05)\n",
    "            yield output[:i+1]\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP_Project\\NLP\\Lib\\site-packages\\gradio\\components\\dropdown.py:93: UserWarning: The `max_choices` parameter is ignored when `multiselect` is False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://f5df6c70e5c3bff838.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f5df6c70e5c3bff838.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(fn=gradio_interface_llm, \n",
    "                        # chatbot = gr.Chatbot([],\n",
    "                        #             elem_id=\"chatbot\",\n",
    "                        #             bubble_full_width=False), \n",
    "                        # examples=[{\"text\": \"Hello, tell me about Chatrapti Shivaji Maharaj\",\n",
    "                        #            \"file\":[]}, \n",
    "                        #            {\"text\": \"Could you give me a brief description about this article\",\n",
    "                        #            \"file\":[]}, \n",
    "                        #            {\"text\": \"Tell me more about the atifact\",\n",
    "                        #            \"file\":[]}, \n",
    "                        #           ],\n",
    "                        additional_inputs= gr.Dropdown(choices=language, \n",
    "                                                       multiselect=False,\n",
    "                                                       max_choices=1\n",
    "                                                       ),\n",
    "                        chatbot = gr.Chatbot([],\n",
    "                                    show_copy_button=True,\n",
    "                                    show_share_button=True,\n",
    "                                    elem_id=\"chatbot\",\n",
    "                                    likeable=True,\n",
    "                                    bubble_full_width=True),\n",
    "                        textbox= gr.MultimodalTextbox(file_types=['image'], \n",
    "                                                      info=\"Type your query here\"),\n",
    "                        title=\"Chatbot - VQA for CSMVS\", \n",
    "                        multimodal=True)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
